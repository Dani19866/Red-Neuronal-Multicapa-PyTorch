# ü§ñ Clasificador Fashion MNIST con PyTorch y GUI

¬°Bienvenido! üëã Este proyecto es una aplicaci√≥n de escritorio con una interfaz gr√°fica (GUI) que te permite crear, cargar, entrenar y evaluar modelos de redes neuronales (MLP) usando **PyTorch**.

El objetivo principal es clasificar im√°genes del popular dataset **Fashion MNIST** üëïüëñüëü.

---

## ‚ú® Caracter√≠sticas Principales

Esta aplicaci√≥n te permite gestionar todo el ciclo de vida de un modelo de clasificaci√≥n:

* **üß† Creaci√≥n de Redes:** Permite crear dos tipos de arquitecturas:
    * **Rectangular (MLP Cl√°sico):** Define el n√∫mero de capas ocultas y neuronas por capa.
    * **Piramidal (MLP con interpolaci√≥n):** Red neuronal multicapas con arquitectura piramidal.
* **üíæ Cargar y Guardar:**
    * Carga un modelo `.pth` previamente entrenado para evaluarlo.
    * Guarda tu modelo reci√©n entrenado en un archivo `.pth`.
* **üìä Entrenamiento Flexible:**
    * Selecciona tus propios archivos `.csv` de entrenamiento y prueba.
    * Define el **optimizador** (Adam, SGD o RMSProp), **tasa de aprendizaje**, **n√∫mero de √©pocas** y **tama√±o del lote** (batch size).
* **üîÑ Doble Modo de Clasificaci√≥n:**
    * **10 Clases:** Clasifica el dataset Fashion MNIST original (Camisa, Pantal√≥n, Bota, etc.).
    * **4 Clases:** Agrupa las 10 clases en 4 categor√≠as simplificadas (Top, Bottom, Calzado, Bolso).
* **üìà Evaluaci√≥n del Modelo:**
    * Visualiza un **gr√°fico de p√©rdida (loss)** por √©poca (Entrenamiento vs. Validaci√≥n).
    * Muestra la **matriz de confusi√≥n** para analizar el rendimiento por clase.
    * Reporta m√©tricas clave: **Accuracy, Precisi√≥n y Recall**.

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

Este proyecto fue construido con las siguientes herramientas y librer√≠as:

* **Python 3**
* **PyTorch:** El framework principal para construir y entrenar las redes neuronales.
* **Tkinter:** Para la construcci√≥n de la interfaz gr√°fica de usuario (GUI).
* **Pandas:** Para la carga y manipulaci√≥n inicial de los datos desde los archivos CSV.
* **Scikit-learn:** Para generar la matriz de confusi√≥n y las m√©tricas de clasificaci√≥n (accuracy, precision, recall).
* **Matplotlib & Seaborn:** Para la visualizaci√≥n de los gr√°ficos de p√©rdida y la matriz de confusi√≥n.
* **Tqdm:** Para mostrar barras de progreso elegantes en la consola durante el entrenamiento.

---

## ‚ö†Ô∏è ¬°Importante! Preparar los Datos

Este repositorio **no** incluye los archivos de datos. Sigue estos pasos para conseguirlos:

1.  Debes descargar el dataset **Fashion MNIST** en formato **CSV**.
2.  Puedes encontrarlo f√°cilmente en [Kaggle (Fashion MNIST)](https://www.kaggle.com/datasets/zalando-research/fashionmnist).
3.  Necesitar√°s los archivos `fashion-mnist_train.csv` y `fashion-mnist_test.csv`.
4.  No necesitas ponerlos en una carpeta espec√≠fica; la aplicaci√≥n te pedir√° que los selecciones usando un di√°logo de archivos.

---

## üìà Entendiendo las M√©tricas y Gr√°ficos

La aplicaci√≥n genera varias visualizaciones para ayudarte a entender el rendimiento de tu modelo. Aqu√≠ tienes una explicaci√≥n detallada de qu√© significa cada una, extra√≠da directamente de la documentaci√≥n del c√≥digo:

### üìä Gr√°fico de P√©rdida (Loss Curve)

* **DEFINICI√ìN:** Este gr√°fico demuestra que tan bien est√° aprendiendo la MLP a lo largo del tiempo, es decir, a lo largo de las √©pocas.
* **Eje X:** Representa la cantidad de √©pocas.
* **Eje Y:** Medida de error. Un n√∫mero alto hace referencia que el modelo est√° cometiendo muchos errores.
* **EXPLICACI√ìN DE L√çNEAS:**
    * **L√≠nea azul (Train Loss o p√©rdida de entrenamiento):** Mide el error del modelo sobre los datos que se est√°n usando para aprender.
    * **L√≠nea roja (Val Loss o p√©rdida de validaci√≥n):** Mide el error del modelo sobre un conjunto de datos nuevos (test dataset). Representa la generalizaci√≥n del modelo.
* **OBJETIVO:** Lograr que las l√≠neas converjan. Si la l√≠nea roja (medici√≥n del modelo vs nuevos datos) se aleja de la l√≠nea azul (medici√≥n del modelo vs datos de entrenamiento), entonces el modelo est√° memorizando, pero no est√° aprendiendo.

### üìâ Matriz de Confusi√≥n

* **DEFINICI√ìN:** La matriz de confusi√≥n es una tabla que compara las etiquetas verdaderas (lo que realmente es) y las etiquetas predichas (lo que predijo el modelo).
* **EJE Y (etiqueta verdadera):** Cada fila representa la etiqueta real. Por ejemplo, la fila "0" contiene todas las im√°genes que realmente eran de la clase "0".
* **EJE X (etiqueta predicha):** Cada columna representa la predicci√≥n del modelo. Por ejemplo, la columna "0" todo lo que el modelo predijo como "0".
* En conclusi√≥n, la matriz nos dice qu√© tan bueno es reconociendo ciertos objetos `[n, n]`, y qu√© tan malo es reconociendo otros `[n, m]`.
* **EXPLICACI√ìN DE LA MATRIZ:** `[fila, columna]`
    * La celda `[1, 1] = 984`. El modelo predijo exitosamente 984 im√°genes de clase "0".
    * La celda `[6, 0] = 142`. El modelo se equivoc√≥ haciendo la predicci√≥n, dijo que hay 142 im√°genes de tipo "0", pero en verdad son de tipo "6".
* **OBJETIVO:** Tener los n√∫meros m√°s altos posibles en la diagonal principal. Tener los n√∫meros m√°s bajos posibles en el resto de la matriz.

### üéØ M√©tricas de Precisi√≥n (Reporte de Clasificaci√≥n)

* **DEFINICI√ìN:** Esto muestra un reporte de clasificaci√≥n, y da un resumen detallado, clase por clase, de qu√© tan bien funciona el modelo.
* **EXPLICACI√ìN DE LAS M√âTRICAS:**
    * **Columna PRECISI√ìN:** De todas las veces que el modelo predijo una clase, ¬øQu√© porcentaje de veces acert√≥?
        * Si tiene una precisi√≥n del 0.61 para la etiqueta "6", significa que de todas las im√°genes que el modelo predijo de etiqueta "6", solo el 61% eran realmente de etiqueta "6". El resto de porcentaje quiere decir el porcentaje de fallos.
    * **Columna RECALL (Sensibilidad, tasa de verdaderos positivos):** La tasa de verdaderos positivos es la proporci√≥n de todos los positivos reales que se clasificaron correctamente como positivos.
        * Un modelo perfecto no tendr√≠a ning√∫n falso negativo (muestras positivas que se clasificaron como negativo), por lo tanto, la tasa de verdaderos positivos ser√≠a 1.0, es decir, detecta el 100% de todas las etiquetas de ese tipo.
        * Un falso negativo tiene m√°s consecuencias que un falso positivo.
    * **PRECISI√ìN vs RECALL:**
        * RECALL calcula el porcentaje de etiquetas positivas que fueron identificadas.
        * PRECISI√ìN: De todas las etiquetas identificadas, ¬øQu√© procentaje es realmente esa etiqueta?
    * **¬øQu√© priorizar?**
        * Priorizar **RECALL** cuando no importa la precisi√≥n. Esto tiene sentido cuando no se permite dejar pasar falsos positivos. Por ejemplo, en una detecci√≥n de c√°ncer, no se permite dejar pasar un caso.
        * Priorizar **PRECISI√ìN** cuando no importa el recall. Esto tiene sentido cuando se quiere tener menos falsos positivos. Por ejemplo, en el caso de un correo electr√≥nico que le llega spam, se quiere tener mayor precisi√≥n, aunque esto signifique mayor probabilidad de correo spam en la bandeja de entrada.
    * **Columna F1-score:**
        * Es una m√©trica que combina la precisi√≥n y el recall en un solo n√∫mero. Es la medida "arm√≥nica" que encuentra el mejor equilibrio posible between esas dos medidas.
        * Un F1-Score es alto solo si tanto la precisi√≥n como el recall son altos.
    * **Columna Support:**
        * El support es la cantidad de muestras totales que existen para cada clase en el conjunto de prueba. Esto solo es un conteo, no es una m√©trica.
    * **Medida de exactitud (Accuracy):**
        * Es la m√©trica m√°s simple de todas. Del total de generalizaciones hechas, ¬øQu√© porcentaje fue correcto?
        * F√≥rmula: `(TOTAL DE ACIERTOS) / (TOTAL DE MUESTRAS)`.
        * Pero esta medida es enga√±osa, se debe combinar con el recall. Por ejemplo, un modelo que predice si una persona tiene c√°ncer y el 99% de los casos dice que no, entonces tiene una exactitud del 99%, pero es in√∫til porque el recall es del 0%.
